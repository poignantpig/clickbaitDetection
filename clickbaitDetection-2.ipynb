{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d860531-18e7-4629-8b37-ae77f361df50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>clickbait</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Should I Get Bings</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Which TV Female Friend Group Do You Belong In</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The New \"Star Wars: The Force Awakens\" Trailer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This Vine Of New York On \"Celebrity Big Brothe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A Couple Did A Stunning Photo Shoot With Their...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31995</th>\n",
       "      <td>To Make Female Hearts Flutter in Iraq, Throw a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31996</th>\n",
       "      <td>British Liberal Democrat Patsy Calton, 56, die...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31997</th>\n",
       "      <td>Drone smartphone app to help heart attack vict...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31998</th>\n",
       "      <td>Netanyahu Urges Pope Benedict, in Israel, to D...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31999</th>\n",
       "      <td>Computer Makers Prepare to Stake Bigger Claim ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                headline  clickbait\n",
       "0                                     Should I Get Bings          1\n",
       "1          Which TV Female Friend Group Do You Belong In          1\n",
       "2      The New \"Star Wars: The Force Awakens\" Trailer...          1\n",
       "3      This Vine Of New York On \"Celebrity Big Brothe...          1\n",
       "4      A Couple Did A Stunning Photo Shoot With Their...          1\n",
       "...                                                  ...        ...\n",
       "31995  To Make Female Hearts Flutter in Iraq, Throw a...          0\n",
       "31996  British Liberal Democrat Patsy Calton, 56, die...          0\n",
       "31997  Drone smartphone app to help heart attack vict...          0\n",
       "31998  Netanyahu Urges Pope Benedict, in Israel, to D...          0\n",
       "31999  Computer Makers Prepare to Stake Bigger Claim ...          0\n",
       "\n",
       "[32000 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv('/Users/nat/Desktop/datasets/clickbait_data.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5b81987-8cf8-44c9-8c46-5ac3ae7c8168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "headline     0\n",
       "clickbait    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73479656-a021-4341-b27b-c6c8d368470e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/nat/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e1a171f-2c4d-43de-92d6-cd87ec588e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def RemoveSpecialCharacters(sentence):\n",
    "    return re.sub('[^a-zA-Z]+',' ',sentence)\n",
    "\n",
    "def ConvertAndRemove(sentence):\n",
    "    sentence = str(sentence)\n",
    "    sentence = RemoveSpecialCharacters(sentence)\n",
    "    # convert to lower case\n",
    "    sentence = ConvertToLowerCase(sentence)\n",
    "    return sentence\n",
    "\n",
    "def ConvertToLowerCase(sentence):\n",
    "    return sentence.lower()\n",
    "\n",
    "def CleanText(sentence):\n",
    "    sentence = str(sentence)\n",
    "\n",
    "    # Remove stopwords\n",
    "    STOPWORDS = stopwords.words('english') + ['u', 'ü', 'ur', '4', '2', 'im', 'dont', 'doin', 'ure']\n",
    "    # Remove punctuation\n",
    "    nopunc = [char for char in sentence if char not in string.punctuation]\n",
    "    nopunc = ''.join(nopunc)\n",
    "    sentence = ' '.join([word for word in nopunc.split() if word.lower() not in STOPWORDS])\n",
    "    sentence = ConvertAndRemove(sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e906f9a-9794-4d0f-989d-73ab5d5662f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zyzzingrizzington bear heem ok sfa lm\n"
     ]
    }
   ],
   "source": [
    "print(CleanText('i am zyzzingrizzington bear i am heem what ok \\asfa[] lm'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f60afa62-85c5-4931-a274-3d4eeb361399",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>clickbait</th>\n",
       "      <th>Text_cleaning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Should I Get Bings</td>\n",
       "      <td>1</td>\n",
       "      <td>get bings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Which TV Female Friend Group Do You Belong In</td>\n",
       "      <td>1</td>\n",
       "      <td>tv female friend group belong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The New \"Star Wars: The Force Awakens\" Trailer...</td>\n",
       "      <td>1</td>\n",
       "      <td>new star wars force awakens trailer give chills</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This Vine Of New York On \"Celebrity Big Brothe...</td>\n",
       "      <td>1</td>\n",
       "      <td>vine new york celebrity big brother fucking pe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A Couple Did A Stunning Photo Shoot With Their...</td>\n",
       "      <td>1</td>\n",
       "      <td>couple stunning photo shoot baby learning inop...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            headline  clickbait   \n",
       "0                                 Should I Get Bings          1  \\\n",
       "1      Which TV Female Friend Group Do You Belong In          1   \n",
       "2  The New \"Star Wars: The Force Awakens\" Trailer...          1   \n",
       "3  This Vine Of New York On \"Celebrity Big Brothe...          1   \n",
       "4  A Couple Did A Stunning Photo Shoot With Their...          1   \n",
       "\n",
       "                                       Text_cleaning  \n",
       "0                                          get bings  \n",
       "1                      tv female friend group belong  \n",
       "2    new star wars force awakens trailer give chills  \n",
       "3  vine new york celebrity big brother fucking pe...  \n",
       "4  couple stunning photo shoot baby learning inop...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Text_cleaning'] = df.headline.apply(CleanText)\n",
    "df.head()\n",
    "# Cleaning dataset done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d42b720-3382-4e3c-9a31-5a64b5db6d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original length training: 32000\n",
      "original length test: 32000\n"
     ]
    }
   ],
   "source": [
    "# need to split datasets\n",
    "X = df.Text_cleaning\n",
    "y = df.clickbait\n",
    "print('original length training:',len(X))\n",
    "print('original length test:',len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "386401c0-8ded-48ba-b829-ab747721a492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new len x_train: 25600\n",
      "new len x_test: 6400\n",
      "new len y_train: 25600\n",
      "new len y_test: 6400\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print('new len x_train:',len(X_train))\n",
    "print('new len x_test:',len(X_test))\n",
    "print('new len y_train:',len(y_train))\n",
    "print('new len y_test:',len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25373fa2-aa51-4cb0-b014-6cc981b95e6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<25600x21436 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 152145 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#vectors\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vect = CountVectorizer()\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "X_test_dtm = vect.transform(X_test)\n",
    "\n",
    "X_train_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32c86fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 20691)\t0.27874184201961916\n",
      "  (0, 17768)\t0.3345529176182634\n",
      "  (0, 15408)\t0.443280710732536\n",
      "  (0, 15002)\t0.37952311039285885\n",
      "  (0, 13192)\t0.443280710732536\n",
      "  (0, 12018)\t0.32318125012329857\n",
      "  (0, 5712)\t0.30945596770049805\n",
      "  (0, 3477)\t0.2704345505044664\n",
      "  (1, 19768)\t0.4943954061330551\n",
      "  (1, 19352)\t0.6624730736498667\n",
      "  (1, 8360)\t0.5627633686403459\n",
      "  (2, 19358)\t0.398093544057493\n",
      "  (2, 14980)\t0.3044588790043617\n",
      "  (2, 14484)\t0.25078631831387677\n",
      "  (2, 13284)\t0.33813632294469903\n",
      "  (2, 12879)\t0.18494510864592098\n",
      "  (2, 12716)\t0.45227553340304516\n",
      "  (2, 9559)\t0.2966918040963563\n",
      "  (2, 7673)\t0.398093544057493\n",
      "  (2, 7100)\t0.2938265186498573\n",
      "  (3, 12037)\t0.31565021664326437\n",
      "  (3, 10624)\t0.41919574997204595\n",
      "  (3, 10118)\t0.402951847353606\n",
      "  (3, 8075)\t0.2854420765479582\n",
      "  (3, 7531)\t0.3409694430081179\n",
      "  :\t:\n",
      "  (25596, 11813)\t0.5860933147732315\n",
      "  (25596, 9071)\t0.3572209408805257\n",
      "  (25596, 7075)\t0.29993546844497326\n",
      "  (25596, 6255)\t0.43653766312518216\n",
      "  (25597, 15954)\t0.36744333021546094\n",
      "  (25597, 15204)\t0.3706426518400437\n",
      "  (25597, 14800)\t0.3565938134684512\n",
      "  (25597, 14186)\t0.2493176952763973\n",
      "  (25597, 13784)\t0.3778545580628872\n",
      "  (25597, 9132)\t0.3565938134684512\n",
      "  (25597, 6704)\t0.3499389612429379\n",
      "  (25597, 4928)\t0.3819692020131808\n",
      "  (25598, 17002)\t0.7052510381922021\n",
      "  (25598, 6398)\t0.47277848738088035\n",
      "  (25598, 2834)\t0.5283005536611407\n",
      "  (25599, 21320)\t0.38062834285086855\n",
      "  (25599, 20877)\t0.2828825355051056\n",
      "  (25599, 17284)\t0.3378991098142329\n",
      "  (25599, 16229)\t0.39022787548077764\n",
      "  (25599, 12609)\t0.29281598602353415\n",
      "  (25599, 11502)\t0.25090142263806064\n",
      "  (25599, 10739)\t0.35534203170308115\n",
      "  (25599, 10390)\t0.23388234199681832\n",
      "  (25599, 9759)\t0.3149781047229859\n",
      "  (25599, 6181)\t0.2824413055997656\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_clean = tfidf_transformer.fit_transform(X_train_dtm)\n",
    "\n",
    "print(X_train_clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1b5dec41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test different models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01e9dc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "Classifiers = [\n",
    "{\n",
    "   'label': 'Logistic Regression',\n",
    "   'model': LogisticRegression(C=0.00000001,solver='liblinear',max_iter=200, multi_class='auto'),\n",
    "},\n",
    "{\n",
    "    'label': 'SGD Classifier',\n",
    "    'model': SGDClassifier(loss='log', warm_start=True, max_iter=1000, l1_ratio=0.03, penalty='l2', alpha=1e-4, fit_intercept=False),\n",
    "},\n",
    "{\n",
    "    'label': 'KNeighbours',\n",
    "    'model': KNeighborsClassifier(n_neighbors=15),\n",
    "},\n",
    "{\n",
    "    'label': 'Decision Tree',\n",
    "    'model': DecisionTreeClassifier(max_depth=10,random_state=101,max_features= None,min_samples_leaf=15),\n",
    "},\n",
    "{\n",
    "   'label': 'Random Forest',\n",
    "   'model': RandomForestClassifier(n_estimators=70, oob_score=True, n_jobs=-1,random_state=101,max_features= None,min_samples_leaf = 30),\n",
    "}\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "913c7912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Logistic Regression is 0.68390625\n",
      "Accuracy of SGD Classifier is 0.9421875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of KNeighbours is 0.8678125\n",
      "Accuracy of Decision Tree is 0.6415625\n",
      "Accuracy of Random Forest is 0.8153125\n"
     ]
    }
   ],
   "source": [
    "Accuracy=[]\n",
    "Model=[]\n",
    "prediction = []\n",
    "for c in Classifiers:\n",
    "    try:\n",
    "        classifier = c['model']\n",
    "        fit = classifier.fit(X_train_clean, y_train)\n",
    "        pred = fit.predict(X_test_dtm)\n",
    "    except Exception:\n",
    "        fit = classifier.fit(X_train_clean, y_train)\n",
    "        pred = fit.predict(X_test_dtm)\n",
    "    prediction.append(pred)\n",
    "    accuracy = accuracy_score(pred,y_test)\n",
    "    Accuracy.append(accuracy)\n",
    "    Model.append(c['label'])\n",
    "    print('Accuracy of '+c['label']+' is '+str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8ef1afee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;bow&#x27;, CountVectorizer()), (&#x27;tfid&#x27;, TfidfTransformer()),\n",
       "                (&#x27;model&#x27;,\n",
       "                 SGDClassifier(fit_intercept=False, l1_ratio=0.03, loss=&#x27;log&#x27;,\n",
       "                               warm_start=True))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;bow&#x27;, CountVectorizer()), (&#x27;tfid&#x27;, TfidfTransformer()),\n",
       "                (&#x27;model&#x27;,\n",
       "                 SGDClassifier(fit_intercept=False, l1_ratio=0.03, loss=&#x27;log&#x27;,\n",
       "                               warm_start=True))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfTransformer</label><div class=\"sk-toggleable__content\"><pre>TfidfTransformer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SGDClassifier</label><div class=\"sk-toggleable__content\"><pre>SGDClassifier(fit_intercept=False, l1_ratio=0.03, loss=&#x27;log&#x27;, warm_start=True)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('bow', CountVectorizer()), ('tfid', TfidfTransformer()),\n",
       "                ('model',\n",
       "                 SGDClassifier(fit_intercept=False, l1_ratio=0.03, loss='log',\n",
       "                               warm_start=True))])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipe = Pipeline([('bow', CountVectorizer()), \n",
    "                 ('tfid', TfidfTransformer()),  \n",
    "                 ('model', Classifiers[1]['model'])])\n",
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1ec9f3a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The headline is Clickbait\n"
     ]
    }
   ],
   "source": [
    "y_pred = pipe.predict(X_test)\n",
    "#sample clickbait from the dataset\n",
    "test_sample1=['The New Star Wars: The Force Awakens Trailer Is Here To Give You Chills'] \n",
    "\n",
    "#sample not a clickbait from the dataset\n",
    "test_sample2 = ['Scientology defector arrested after attempting to leave organization']  \n",
    "\n",
    "#made up sample, expected to be clickbait\n",
    "test_sample3 =['The Ultimate Guide to Making Money Online']\n",
    "\n",
    "prediction = pipe.predict(test_sample3)\n",
    "if prediction == 0:\n",
    "  result = 'Not Clickbait'\n",
    "else:\n",
    "    result = 'Clickbait'\n",
    "print(f'The headline is {result}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e654fcad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

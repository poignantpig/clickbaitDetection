{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d860531-18e7-4629-8b37-ae77f361df50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>clickbait</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Should I Get Bings</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Which TV Female Friend Group Do You Belong In</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The New \"Star Wars: The Force Awakens\" Trailer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This Vine Of New York On \"Celebrity Big Brothe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A Couple Did A Stunning Photo Shoot With Their...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31995</th>\n",
       "      <td>To Make Female Hearts Flutter in Iraq, Throw a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31996</th>\n",
       "      <td>British Liberal Democrat Patsy Calton, 56, die...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31997</th>\n",
       "      <td>Drone smartphone app to help heart attack vict...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31998</th>\n",
       "      <td>Netanyahu Urges Pope Benedict, in Israel, to D...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31999</th>\n",
       "      <td>Computer Makers Prepare to Stake Bigger Claim ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                headline  clickbait\n",
       "0                                     Should I Get Bings          1\n",
       "1          Which TV Female Friend Group Do You Belong In          1\n",
       "2      The New \"Star Wars: The Force Awakens\" Trailer...          1\n",
       "3      This Vine Of New York On \"Celebrity Big Brothe...          1\n",
       "4      A Couple Did A Stunning Photo Shoot With Their...          1\n",
       "...                                                  ...        ...\n",
       "31995  To Make Female Hearts Flutter in Iraq, Throw a...          0\n",
       "31996  British Liberal Democrat Patsy Calton, 56, die...          0\n",
       "31997  Drone smartphone app to help heart attack vict...          0\n",
       "31998  Netanyahu Urges Pope Benedict, in Israel, to D...          0\n",
       "31999  Computer Makers Prepare to Stake Bigger Claim ...          0\n",
       "\n",
       "[32000 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv('/Users/nat/Desktop/datasets/clickbait_data.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5b81987-8cf8-44c9-8c46-5ac3ae7c8168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "headline     0\n",
       "clickbait    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73479656-a021-4341-b27b-c6c8d368470e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/nat/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5e1a171f-2c4d-43de-92d6-cd87ec588e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def RemoveSpecialCharacters(sentence):\n",
    "    return re.sub('[^a-zA-Z]+',' ',sentence)\n",
    "\n",
    "def ConvertAndRemove(sentence):\n",
    "    sentence = str(sentence)\n",
    "    sentence = RemoveSpecialCharacters(sentence)\n",
    "    # convert to lower case\n",
    "    sentence = ConvertToLowerCase(sentence)\n",
    "    return sentence\n",
    "\n",
    "def ConvertToLowerCase(sentence):\n",
    "    return sentence.lower()\n",
    "\n",
    "def CleanText(sentence):\n",
    "    sentence = str(sentence)\n",
    "\n",
    "    # Remove stopwords\n",
    "    STOPWORDS = stopwords.words('english') + ['u', 'ü', 'ur', '4', '2', 'im', 'dont', 'doin', 'ure']\n",
    "    # Remove punctuation\n",
    "    nopunc = [char for char in sentence if char not in string.punctuation]\n",
    "    nopunc = ''.join(nopunc)\n",
    "    sentence = ' '.join([word for word in nopunc.split() if word.lower() not in STOPWORDS])\n",
    "    sentence = ConvertAndRemove(sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2e906f9a-9794-4d0f-989d-73ab5d5662f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zyzzingrizzington bear heem ok sfa lm\n"
     ]
    }
   ],
   "source": [
    "print(CleanText('i am zyzzingrizzington bear i am heem what ok \\asfa[] lm'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f60afa62-85c5-4931-a274-3d4eeb361399",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>clickbait</th>\n",
       "      <th>Text_cleaning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Should I Get Bings</td>\n",
       "      <td>1</td>\n",
       "      <td>get bings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Which TV Female Friend Group Do You Belong In</td>\n",
       "      <td>1</td>\n",
       "      <td>tv female friend group belong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The New \"Star Wars: The Force Awakens\" Trailer...</td>\n",
       "      <td>1</td>\n",
       "      <td>new star wars force awakens trailer give chills</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This Vine Of New York On \"Celebrity Big Brothe...</td>\n",
       "      <td>1</td>\n",
       "      <td>vine new york celebrity big brother fucking pe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A Couple Did A Stunning Photo Shoot With Their...</td>\n",
       "      <td>1</td>\n",
       "      <td>couple stunning photo shoot baby learning inop...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            headline  clickbait   \n",
       "0                                 Should I Get Bings          1  \\\n",
       "1      Which TV Female Friend Group Do You Belong In          1   \n",
       "2  The New \"Star Wars: The Force Awakens\" Trailer...          1   \n",
       "3  This Vine Of New York On \"Celebrity Big Brothe...          1   \n",
       "4  A Couple Did A Stunning Photo Shoot With Their...          1   \n",
       "\n",
       "                                       Text_cleaning  \n",
       "0                                          get bings  \n",
       "1                      tv female friend group belong  \n",
       "2    new star wars force awakens trailer give chills  \n",
       "3  vine new york celebrity big brother fucking pe...  \n",
       "4  couple stunning photo shoot baby learning inop...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Text_cleaning'] = df.headline.apply(CleanText)\n",
    "df.head()\n",
    "# Cleaning dataset done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6d42b720-3382-4e3c-9a31-5a64b5db6d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original length training: 32000\n",
      "original length test: 32000\n"
     ]
    }
   ],
   "source": [
    "# need to split datasets\n",
    "X = df.Text_cleaning\n",
    "y = df.clickbait\n",
    "print('original length training:',len(X))\n",
    "print('original length test:',len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "386401c0-8ded-48ba-b829-ab747721a492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new len x_train: 25600\n",
      "new len x_test: 6400\n",
      "new len y_train: 25600\n",
      "new len y_test: 6400\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print('new len x_train:',len(X_train))\n",
    "print('new len x_test:',len(X_test))\n",
    "print('new len y_train:',len(y_train))\n",
    "print('new len y_test:',len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25373fa2-aa51-4cb0-b014-6cc981b95e6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
